{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d8bdcd3-a972-4d26-a42e-a325fae10db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# stores all files as series\n",
    "files_as_series = {}\n",
    "# stores first sentence of all files\n",
    "first_sentence_file = {}\n",
    "# stores all POS files as series\n",
    "pos_files_as_series = {}\n",
    "\n",
    "\n",
    "# get index of each token in the .txt file this token is from and returns all indexes as a list.\n",
    "def get_indexes_as_series(tokens):\n",
    "    return tokens.index.tolist()\n",
    "    \n",
    "# returns a list of this value of size n, where each value is the \n",
    "# length of the .txt file this token is from, where n is the length of the input tokens series\n",
    "def get_len_as_series(tokens):\n",
    "    temp = [len(tokens) for i in range(0, len(tokens))]\n",
    "    return temp\n",
    "\n",
    "\n",
    "def file_to_series(file_name):    \n",
    "    # Source: https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/\n",
    "    with open(file_name) as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return pd.Series(lines)\n",
    "\n",
    "# Strip PubMed IDs from file names.\n",
    "strip_pmid = lambda x: x.split(\".\")[0]\n",
    "\n",
    "def iter_token_dir(dir_name, df, label_dict, col_name = \"Token\", ext_name = \".tokens\"):\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            \n",
    "            pos_series_file = file_to_series(directory.decode(\"utf-8\") + file_name.split(\".\")[0] + \".pos\")\n",
    "            \n",
    "            files_as_series[file_name] = series_file\n",
    "            pos_files_as_series[file_name.split(\".\")[0] + \".pos\"] = pos_series_file\n",
    "            \n",
    "            token_index = get_indexes_as_series(series_file)\n",
    "            file_len = get_len_as_series(series_file)\n",
    "                        \n",
    "            PMID = strip_pmid(file_name)\n",
    "            df_file = pd.DataFrame({col_name: series_file,\n",
    "                                    \"File\": [file_name] * len(series_file),\n",
    "                                    \"Gold\": label_dict.get(PMID),\n",
    "                                    \"PMID\": [PMID] * len(series_file),\n",
    "                                    \"token_index\": token_index,\n",
    "                                    \"file_len\": file_len\n",
    "                                   })\n",
    "            df = pd.concat([df, df_file])\n",
    "        else:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "def iter_label_dir(dir_name, ext_name = \".AGGREGATED.ann\"):\n",
    "    label_dict = dict()\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            PMID = strip_pmid(file_name)\n",
    "            label_dict[PMID] = series_file\n",
    "        else:\n",
    "            continue\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "# get sentence index, sentence, and parts of speech of sentence, that token is in\n",
    "def get_sentence_info(token_index, file_name):\n",
    "        \n",
    "    token_series = files_as_series[file_name]\n",
    "    pos_series = pos_files_as_series[file_name.split(\".\")[0] + \".pos\"]\n",
    "    sentence = []\n",
    "    pos_sentence = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    i = token_index\n",
    "    \n",
    "    if token_series[i]=='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "        \n",
    "    while i>=0 and token_series[i]!='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "\n",
    "    # index within sentence\n",
    "    sentence_index = token_index - (i+1)\n",
    "    i = token_index+1 if token_series[token_index]!='.' else token_index\n",
    "\n",
    "    while i<len(token_series) and token_series[i]!='.':\n",
    "        sentence.append(token_series[i])\n",
    "        pos_sentence.append(pos_series[i])\n",
    "        i+=1\n",
    "\n",
    "    if token_index==0:\n",
    "        first_sentence_file[file_name] = [x.lower() for x in sentence]\n",
    "            \n",
    "    return (token_series.tolist(), pos_series.tolist())\n",
    "\n",
    "\n",
    "def get_abstract(x):\n",
    "    s, ps = x\n",
    "    return s\n",
    "def get_pos_abstract(x):\n",
    "    s, ps = x\n",
    "    return ps\n",
    "\n",
    "\n",
    "# tokens that are punctuation.\n",
    "def is_punctuation(x):\n",
    "    return False if x.Token.lower() in string.punctuation else True\n",
    "\n",
    "# Iterate through directory to obtain all gold labels, \n",
    "# mapped to their respective file names.\n",
    "label_dict = iter_label_dir(\"annotations/aggregated/starting_spans/interventions/train/\")\n",
    "\n",
    "# Iterate through directory to obtain all tokens,\n",
    "# mapped to their respective file names.\n",
    "# original tokens\n",
    "df_orig = pd.DataFrame()\n",
    "df_orig = iter_token_dir(\"documents/\", df_orig, label_dict)\n",
    "\n",
    "# get sentence related columns for each token\n",
    "df_orig[\"sentence_info\"] = df_orig.apply(lambda x : get_sentence_info(x[\"token_index\"], x[\"File\"]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d7e67ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentence_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentence_info'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-003ffab2af50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_abstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_abstract\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pos_abstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentence_info'"
     ]
    }
   ],
   "source": [
    "df_orig[\"abstract\"] = df_orig[\"sentence_info\"].apply(get_abstract)\n",
    "df_orig[\"pos_abstract\"] = df_orig[\"sentence_info\"].apply(get_pos_abstract)\n",
    "\n",
    "df_orig = df_orig.drop(\"sentence_info\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0150a5de-8c8d-46a3-acc0-bc0f5d3a08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.reset_index(drop=True)\n",
    "\n",
    "df_orig = df_orig.head(708703)\n",
    "\n",
    "df_orig.to_pickle('df_orig.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82042a54-d694-4ce3-9c7a-7b0015658224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_sentence_file.pickle', 'wb') as handle:\n",
    "    pickle.dump(first_sentence_file, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb7fc69b-e6a1-44c2-a392-6274c80d19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2808984\n",
      "              Token            File Gold     PMID  token_index  file_len  \\\n",
      "0    Rate-modulated  2808984.tokens    0  2808984            0       317   \n",
      "1           cardiac  2808984.tokens    0  2808984            1       317   \n",
      "2            pacing  2808984.tokens    0  2808984            2       317   \n",
      "3             based  2808984.tokens    0  2808984            3       317   \n",
      "4                on  2808984.tokens    0  2808984            4       317   \n",
      "..              ...             ...  ...      ...          ...       ...   \n",
      "312       TRUNCATED  2808984.tokens    0  2808984          312       317   \n",
      "313              AT  2808984.tokens    0  2808984          313       317   \n",
      "314             250  2808984.tokens    0  2808984          314       317   \n",
      "315           WORDS  2808984.tokens    0  2808984          315       317   \n",
      "316               )  2808984.tokens    0  2808984          316       317   \n",
      "\n",
      "                                              abstract  \\\n",
      "0    [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "1    [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "2    [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "3    [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "4    [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "..                                                 ...   \n",
      "312  [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "313  [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "314  [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "315  [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "316  [Rate-modulated, cardiac, pacing, based, on, t...   \n",
      "\n",
      "                                          pos_abstract  \n",
      "0    [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "1    [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "2    [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "3    [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "4    [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "..                                                 ...  \n",
      "312  [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "313  [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "314  [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "315  [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "316  [JJ, NN, NN, VBN, IN, JJ, NN, NNS, IN, JJ, NN,...  \n",
      "\n",
      "[317 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "i = 2000\n",
    "for name, groups in df_orig.groupby('PMID'):\n",
    "    if i == 0:\n",
    "        print(name)\n",
    "        print(groups)\n",
    "    i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b04015bf-a2b7-45c5-a7a3-da7799773a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.iloc[10021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596254f-9b1b-4b00-b2eb-2288fbfdb583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
