{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcb4bb9-12de-4189-85a3-3320ccf9be85",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data programming with Snorkel: Labeling the PICO dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e0a69cf-15ef-45ee-8861-bb873fce2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importations.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snorkel\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.analysis import get_label_buckets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string # For punctuation.\n",
    "import os\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9baa17d-5e0c-4a07-b8a6-d52a309d24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_sentence_file.pickle', 'rb') as handle:\n",
    "    first_sentence_file = pickle.load(handle)\n",
    "    \n",
    "# meeting notes:\n",
    "\n",
    "# LF idea:\n",
    "# many false positives in LF results\n",
    "# predicting tokens as I when not I, ex. is treatment\n",
    "# is there 'treatment' as a I in any abstracts?\n",
    "    # should probably remove 'treatment' from LF\n",
    "# see if performance is improved\n",
    "\n",
    "# to develop LFs:\n",
    "# 1. need to create LFs by looking at abstracts closely, what words are being labeled as I?\n",
    "# 2. browse through ground truth files\n",
    "# 3. encode patterns into LFs\n",
    "# 4. better to have held out development set for developing LFs separate from training/testing, look at 100-200, and observe patterns. \n",
    "\n",
    "# to see if LFs worked:\n",
    "# 1. run LFs on same abstracts in snorkel again and again, look at error generation files, if it's catching those errors or not.\n",
    "# 2. look at each LF's performacen metrics, indiviudally too, in addition to looking at overall performance.\n",
    "# 3. check if bert labels those tokens as I, if it can't then maybe just not possible to catch that token as an I\n",
    "\n",
    "\n",
    "# share what LFs are being used on slack and in github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73489cc0-4414-4ad9-ab40-ffd85d3f34ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process keyword data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dc43e5-51bc-4981-9ae5-083734b6a32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in suffixes.\n",
    "# https://druginfo.nlm.nih.gov/drugportal/jsp/drugportal/DrugNameGenericStems.jsp\n",
    "df_drugs = pd.read_csv(\"lf_datasets/suffixes/drug_suffixes.txt\", header = None)\n",
    "df_surgery = pd.read_csv(\"lf_datasets/suffixes/surgical_suffixes.txt\", header = None)\n",
    "df_psych = pd.read_csv(\"lf_datasets/suffixes/psychotherapy_keywords.txt\", header = None)\n",
    "\n",
    "df_psych[0] = df_psych[0].str.lower()\n",
    "df_surgery[0] = df_surgery[0].str.lower()\n",
    "df_drugs[0] = df_drugs[0].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a4a89e-08f7-48c3-b318-d9b82dbed512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-d68a5213957a>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df_drugs_at_fda = pd.read_csv(\"lf_datasets/fda_approved_drugs/products_drugs_at_fda.txt\",\n",
      "b'Skipping line 35225: expected 8 fields, saw 9\\nSkipping line 35226: expected 8 fields, saw 9\\nSkipping line 35227: expected 8 fields, saw 9\\n'\n"
     ]
    }
   ],
   "source": [
    "# Read in FDA data.\n",
    "\n",
    "df_purple = pd.read_csv(\"lf_datasets/fda_approved_drugs/products_purplebook.csv\")\n",
    "df_orange = pd.read_csv(\"lf_datasets/fda_approved_drugs/products_orangebook.txt\", \n",
    "                        sep = \"~\")\n",
    "df_drugs_at_fda = pd.read_csv(\"lf_datasets/fda_approved_drugs/products_drugs_at_fda.txt\", \n",
    "                              sep = \"\\t\", \n",
    "                              error_bad_lines = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65cb622-487c-4872-ab25-87399bfd84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter drug suffixes with three characters or fewer.\n",
    "drug_suffixes = list(df_drugs[0])\n",
    "drug_suffixes = [x.lower() for x in drug_suffixes if len(x) > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73432812-6063-449f-bfbd-2e8756a4b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate FDA drug data.\n",
    "set_proprietary = list(df_drugs_at_fda[\"DrugName\"]) + list(df_purple[\"Proprietary Name\"]) + list(df_orange[\"Trade_Name\"])\n",
    "set_proper = list(df_drugs_at_fda[\"ActiveIngredient\"]) + list(df_purple[\"Proper Name\"]) + list(df_orange[\"Ingredient\"])\n",
    "\n",
    "# Remove floats and integers.\n",
    "set_proprietary = [item.lower() for item in set_proprietary if not isinstance(item, float)]\n",
    "set_proprietary = [item for item in set_proprietary if not isinstance(item, int)]\n",
    "set_proper = [item.lower() for item in set_proper if not isinstance(item, float)]\n",
    "set_proper = [item for item in set_proper if not isinstance(item, int)]\n",
    "\n",
    "# Cast as sets to remove duplicates.\n",
    "set_proprietary = set(set_proprietary)\n",
    "set_proper = set(set_proper)\n",
    "set_fda = set.union(set_proprietary, set_proper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2442c7-c67d-4929-99aa-45f5150a6141",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcb22ea-dfc7-4bec-bfd3-89ecfe78f5b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the pickle file\n",
    "with open('df_dev.pickle', 'rb') as handle:\n",
    "    df_dev_orig = pickle.load(handle)\n",
    "\n",
    "# Remove None type labels. BERT does the same.\n",
    "df_dev = df_dev_orig.dropna()\n",
    "df_dev = df_dev.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91caec48-6839-4e1f-b32a-4bcf63765253",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Labeling functions\n",
    "\n",
    "Labeling functions will be written to cover the following intervention categories, as used by the manual annotators of this [dataset](https://github.com/bepnye/EBM-NLP):\n",
    "\n",
    "- Surgical.\n",
    "- Physical.\n",
    "- Drug.\n",
    "- Educational.\n",
    "- Psychological.\n",
    "- Other.\n",
    "- Control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c681538f-de23-409a-a9fd-311493e99fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words = 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rcw258/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Label macros.\n",
    "ABSTAIN = -1\n",
    "NOT_I = 0\n",
    "I = 1\n",
    "\n",
    "# Data for labeling functions.\n",
    "generic_interventions = [\"pretreatment\", \"placebo\"]\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(\"Total stop words =\", len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a3071-c24f-4336-9192-65f1ab5dfe9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Labeling functions\n",
    "**All labeling functions label tokens. The corresponding gold labels are from the \"starting span\" labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b3292de-a337-4cf2-9976-629fafae9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling function for tokens, if token is present in first sentence (title)\n",
    "@labeling_function(resources=dict(first_sentence_file=first_sentence_file))\n",
    "def in_title(x, first_sentence_file):\n",
    "    return I if x.token.lower() in first_sentence_file[x.file] else ABSTAIN\n",
    "# abstain or not_i?\n",
    "\n",
    "# Labeling function for tokens, if token is present in first sentence (title)\n",
    "@labeling_function(resources=dict(first_sentence_file=first_sentence_file))\n",
    "def in_title2(x, first_sentence_file):\n",
    "    return I if x.token.lower() in first_sentence_file[x.file] else ABSTAIN\n",
    "# abstain or not_i?\n",
    "\n",
    "# Labeling function for tokens, if token is present in first sentence (title)\n",
    "@labeling_function(resources=dict(first_sentence_file=first_sentence_file))\n",
    "def in_title3(x, first_sentence_file):\n",
    "    return I if x.token.lower() in first_sentence_file[x.file] else ABSTAIN\n",
    "# abstain or not_i?\n",
    "\n",
    "# Labeling function for tokens, if token is present in first sentence (title)\n",
    "@labeling_function(resources=dict(first_sentence_file=first_sentence_file))\n",
    "def in_title4(x, first_sentence_file):\n",
    "    return I if x.token.lower() in first_sentence_file[x.file] else ABSTAIN\n",
    "# abstain or not_i?\n",
    "\n",
    "\n",
    "# Labeling function for tokens, if token is present in first sentence (title)\n",
    "@labeling_function(resources=dict(first_sentence_file=first_sentence_file))\n",
    "def not_in_title(x, first_sentence_file):\n",
    "    return NOT_I if x.token.lower() not in first_sentence_file[x.file] else ABSTAIN\n",
    "# abstain or not_i?\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(first_sentence_file=first_sentence_file))\n",
    "def surround_in_title(x, first_sentence_file):\n",
    "    if((x.token_index>0) and (x.token_index<len(x.abstract)-1)):\n",
    "        if (x.abstract[x.token_index-1].lower() in first_sentence_file[x.file]) and (x.abstract[x.token_index+1].lower() in first_sentence_file[x.file]):\n",
    "            return I\n",
    "    \n",
    "    return ABSTAIN\n",
    "# abstain or not_i?\n",
    "\n",
    "\n",
    "# Labeling function for tokens that contain drug suffixes.\n",
    "@labeling_function()\n",
    "def contains_drug_suffix(x):\n",
    "    return I if (any(suffix.lower() in x.token.lower() for suffix in drug_suffixes)) else ABSTAIN\n",
    "    \n",
    "# Labeling function for tokens that contain surgical suffixes.\n",
    "@labeling_function()\n",
    "def contains_surgical_suffix(x):\n",
    "    return I if (any(suffix.lower() in x.token.lower() for suffix in df_surgery[0])) else ABSTAIN\n",
    "\n",
    "# Labeling function for tokens that contain psychological / psychotherapeutic keywords.\n",
    "@labeling_function()\n",
    "def contains_psych_term(x):\n",
    "    return I if (any(suffix.lower() in x.token.lower() for suffix in df_psych[0])) else ABSTAIN\n",
    "\n",
    "# Labeling function for tokens that contain generic intervention keywords.\n",
    "@labeling_function()\n",
    "def is_generic(x):\n",
    "    return I if (any(term.lower() in x.token.lower() for term in generic_interventions)) else ABSTAIN\n",
    "\n",
    "# Labeling function for tokens that are 'placebo'\n",
    "@labeling_function()\n",
    "def is_placebo(x):\n",
    "    return I if 'placebo' in x.token.lower() else ABSTAIN\n",
    "\n",
    "# Labeling function for stop words.\n",
    "@labeling_function()\n",
    "def is_stop_word(x):\n",
    "    return NOT_I if x.token.lower() in stop_words else ABSTAIN\n",
    "\n",
    "# Labeling function for tokens that are punctuation.\n",
    "@labeling_function()\n",
    "def is_punctuation(x):\n",
    "    return NOT_I if x.token.lower() in string.punctuation else ABSTAIN\n",
    "\n",
    "\n",
    "# Labeling function for FDA approved drugs.\n",
    "@labeling_function()\n",
    "def contains_fda_drug(x):\n",
    "    if (len(x.token) <= 5):\n",
    "        return ABSTAIN\n",
    "\n",
    "    return I if (any(x.token.lower() in drug.lower() for drug in set_fda)) else ABSTAIN\n",
    "\n",
    "\n",
    "# checks if the preceding token is 'of' or 'with' (effect of... I, treat with... I)\n",
    "@labeling_function()\n",
    "def has_prev_word_as(x):\n",
    "    words = ['of', 'with', 'receive', 'and']\n",
    "    if ((x.token_index > 0) and (x.abstract[x.token_index-1].lower() in words)):\n",
    "        return I \n",
    "\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "# checks if the next token is 'group' or 'groups'\n",
    "@labeling_function()\n",
    "def has_next_word_as(x):\n",
    "    words = ['group', 'groups']\n",
    "    if ((x.token_index < len(x.abstract)-1) and (x.abstract[x.token_index+1].lower() in words)):\n",
    "        return NOT_I\n",
    "\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "# Labeling function for tokens, sees if left span of token within sentence contains keyword\n",
    "@labeling_function()\n",
    "def left_span_contains(x):\n",
    "    \n",
    "    i = 0\n",
    "    while(x.abstract[i] != x.token):\n",
    "        i+=1\n",
    "        \n",
    "    count = 0\n",
    "    while(i >= 0 and count < 10):\n",
    "        if((x.abstract[i] == 'determine') or (x.abstract[i] == 'assess')):\n",
    "            return I\n",
    "        i-=1\n",
    "        count+=1\n",
    "        \n",
    "    return ABSTAIN\n",
    "# look into spouse tutorial left spans, and using 'resources' in LFs\n",
    "\n",
    "\n",
    "# checks if the preceding token is VBD, VBN (e.g. was administered)\n",
    "@labeling_function()\n",
    "def right_span_vb_pos(x):\n",
    "    if (x.token_index < len(x.abstract) - 2) and (x.pos_abstract[x.token_index+1] == 'VBD') and (x.pos_abstract[x.token_index+2] == 'VBN'):\n",
    "        return I \n",
    "\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "    \n",
    "# checks if the preceding token is VBD, VBN (e.g. was administered)\n",
    "@labeling_function()\n",
    "def left_span_vb_pos(x):\n",
    "    if (x.token_index > 0) and ('V' in x.pos_abstract[x.token_index-1]):\n",
    "        return I\n",
    "\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "\n",
    "# Labeling function which labels a token as NOT_I if it is in the last 50% of the file tokens.\n",
    "@labeling_function()\n",
    "def has_high_idx(x):\n",
    "    percent = x.token_index / x.file_len\n",
    "    if percent > 0.50:\n",
    "        return NOT_I\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "    \n",
    "# Labeling function which labels a token as NOT_I as default.\n",
    "@labeling_function()\n",
    "def lf_not_i(x):\n",
    "    return NOT_I\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e186e6-5b55-42d8-a953-73acdb04e2bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f5db4a6-5e97-42ff-97c2-5024adffbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19246/19246 [00:00<00:00, 20631.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply LFs to dataframe.\n",
    "lfs = [\n",
    "       #contains_psych_term, # accuracy = 0.131380\n",
    "       # is_punctuation,\n",
    "       #has_prev_word_as,\n",
    "    \n",
    "       # has_next_word_as_drug, low accuracy and coverage\n",
    "    \n",
    "       # left_span_contains,\n",
    "       # right_span_vb_pos,\n",
    "       # left_span_vb_pos,\n",
    "    \n",
    "       #negative LFs\n",
    "       is_stop_word,\n",
    "       has_next_word_as,\n",
    "       has_high_idx,\n",
    "#        lf_not_i,\n",
    "        \n",
    "    \n",
    "       #positive LFs\n",
    "#        is_generic,\n",
    "#        contains_drug_suffix,\n",
    "#        contains_surgical_suffix,\n",
    "#        is_placebo,\n",
    "#        contains_fda_drug,\n",
    "    \n",
    "#        in_title,\n",
    "#         in_title2,\n",
    "#         in_title3,\n",
    "#         in_title4,\n",
    "       # not_in_title,\n",
    "#        surround_in_title,\n",
    "\n",
    "      ]\n",
    "applier = PandasLFApplier(lfs = lfs)\n",
    "L_dev = applier.apply(df = df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c6ea8c7-e289-4499-81b7-524612418c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Y_dev.\n",
    "Y_dev = df_dev[\"gold\"].to_numpy(dtype = int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac65f849-308f-4444-a89a-c0a3324c7510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/anaconda3/2021.05/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[-1  0  1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "/share/apps/anaconda3/2021.05/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[-1  0  1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "/share/apps/anaconda3/2021.05/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=[-1  0  1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Incorrect</th>\n",
       "      <th>Emp. Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_stop_word</th>\n",
       "      <td>0</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.279435</td>\n",
       "      <td>0.142731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5170</td>\n",
       "      <td>208</td>\n",
       "      <td>0.961324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_next_word_as</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.009145</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131</td>\n",
       "      <td>45</td>\n",
       "      <td>0.744318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_high_idx</th>\n",
       "      <td>2</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.497454</td>\n",
       "      <td>0.147615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9183</td>\n",
       "      <td>391</td>\n",
       "      <td>0.959160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
       "is_stop_word      0      [0]  0.279435  0.142731        0.0     5170   \n",
       "has_next_word_as  1      [0]  0.009145  0.006599        0.0      131   \n",
       "has_high_idx      2      [0]  0.497454  0.147615        0.0     9183   \n",
       "\n",
       "                  Incorrect  Emp. Acc.  \n",
       "is_stop_word            208   0.961324  \n",
       "has_next_word_as         45   0.744318  \n",
       "has_high_idx            391   0.959160  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarive coverage, conflicts, empirical accurcacy of LFs.\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08007c6b-893e-43da-81fa-c8e20261e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority vote model.\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L = L_dev)\n",
    "\n",
    "# Label model.\n",
    "label_model = LabelModel(cardinality = 2, verbose = True)\n",
    "label_model.fit(L_train = L_dev, n_epochs = 500, log_freq = 100, seed = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c39a76f-37fa-4cd6-bf9a-8bb6c812538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate error files\n",
    "\n",
    "df_dev['maj_pred']=pd.Series(preds_train.astype(str))\n",
    "\n",
    "df_dev_orig['maj_pred'] = '-2'\n",
    "\n",
    "# adding maj_pred column to dev_orig\n",
    "for i in range(len(df_dev)):\n",
    "    pmid = df_dev.loc[i, 'PMID']\n",
    "    token_index = df_dev.loc[i, 'token_index']\n",
    "    maj_pred = df_dev.loc[i, 'maj_pred']\n",
    "    ind = df_dev_orig.index[((df_dev_orig['PMID'] == pmid) & (df_dev_orig['token_index'] == token_index))][0]\n",
    "    df_dev_orig.loc[ind, 'maj_pred'] = maj_pred\n",
    "    \n",
    "    \n",
    "\n",
    "df_dev_groups = df_dev_orig.groupby('PMID')\n",
    "\n",
    "# outputting sample files, with tokens that are gold labeled as I highlighted\n",
    "for name, group in df_dev_groups:\n",
    "    with open('error/dev/gold/' + name + '.md', 'w') as output:\n",
    "        output.write(' GOLD ')\n",
    "        tokens = group.token.tolist()\n",
    "        golds = group.gold.tolist()\n",
    "        for i in range(0, len(tokens)):\n",
    "            if(golds[i]=='1'):\n",
    "                output.write('**' + tokens[i] + '** ')\n",
    "            else:\n",
    "                output.write(tokens[i] + ' ')\n",
    "            \n",
    "# outputting sample files, with tokens that are incorrectly labeled as intervention (tokens gold labeled as 0, but predicted as 1)\n",
    "for name, group in df_dev_groups:\n",
    "    with open('error/dev/incorrect/' + name + '.md', 'w') as output:\n",
    "        output.write(' INCORRECT ')\n",
    "        tokens = group.token.tolist()\n",
    "        golds = group.gold.tolist()\n",
    "        preds = group.maj_pred.tolist()\n",
    "        for i in range(0, len(tokens)):\n",
    "            if((golds[i]=='0') and (preds[i]=='1')):\n",
    "                output.write('**' + tokens[i] + '** ')\n",
    "            else:\n",
    "                output.write(tokens[i] + ' ')\n",
    "            \n",
    "# outputting sample files, with tokens that are missed highlighted (tokens gold labeled as 1, but predicted as 0 or -1)\n",
    "for name, group in df_dev_groups:\n",
    "    with open('error/dev/missed/' + name + '.md', 'w') as output:\n",
    "        output.write(' MISSED ')\n",
    "        tokens = group.token.tolist()\n",
    "        golds = group.gold.tolist()\n",
    "        preds = group.maj_pred.tolist()\n",
    "        for i in range(0, len(tokens)):\n",
    "            if((golds[i]=='1') and ((preds[i]=='-1') or (preds[i]=='0'))):\n",
    "                output.write('**' + tokens[i] + '** ')\n",
    "            else:\n",
    "                output.write(tokens[i] + ' ')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98745ac8-e558-4350-aff7-beb7cedb758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|          | 0/500 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.097]\n",
      "INFO:root:[100 epochs]: TRAIN:[loss=0.000]\n",
      " 35%|███▌      | 177/500 [00:00<00:00, 1765.82epoch/s]INFO:root:[200 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[300 epochs]: TRAIN:[loss=0.000]\n",
      " 72%|███████▏  | 360/500 [00:00<00:00, 1798.03epoch/s]INFO:root:[400 epochs]: TRAIN:[loss=0.000]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1796.25epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Compute model performance metrics.\n",
    "majority_scores = majority_model.score(L = L_dev, Y = Y_dev, \n",
    "                                       tie_break_policy = \"random\",\n",
    "                                       metrics = [\"f1\", \"accuracy\", \"precision\", \n",
    "                                                  \"recall\", \"roc_auc\", \"coverage\"])\n",
    "\n",
    "label_scores = label_model.score(L = L_dev, Y = Y_dev, \n",
    "                                 tie_break_policy = \"random\",\n",
    "                                 metrics = [\"f1\", \"accuracy\", \"precision\", \n",
    "                                            \"recall\", \"roc_auc\", \"coverage\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad7a6fb6-4119-4b0c-91a4-db1ce809f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Model F1:        20.8%\n",
      "Majority Model Accuracy:  79.2%\n",
      "Majority Model Precision: 15.4%\n",
      "Majority Model Recall:    32.1%\n",
      "Majority Model AUC ROC:   65.6%\n",
      "Majority Model Coverage:  100.0%\n",
      "++++++++++++++++++++++++\n",
      "Label Model F1:           20.8%\n",
      "Label Model Accuracy:     79.2%\n",
      "Label Model Precision:    15.4%\n",
      "Label Model Recall:       32.1%\n",
      "Label Model AUC ROC:      67.0%\n",
      "Label Model Coverage:     100.0%\n"
     ]
    }
   ],
   "source": [
    "# Compare model performance metrics.\n",
    "majority_f1 = majority_scores.get(\"f1\")\n",
    "majority_acc = majority_scores.get(\"accuracy\")\n",
    "majority_prec = majority_scores.get(\"precision\")\n",
    "majority_rec = majority_scores.get(\"recall\")\n",
    "majority_roc = majority_scores.get(\"roc_auc\")\n",
    "majority_cov = majority_scores.get(\"coverage\")\n",
    "print(f\"{'Majority Model F1:':<25} {majority_f1 * 100:.1f}%\")\n",
    "print(f\"{'Majority Model Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "print(f\"{'Majority Model Precision:':<25} {majority_prec * 100:.1f}%\")\n",
    "print(f\"{'Majority Model Recall:':<25} {majority_rec * 100:.1f}%\")\n",
    "print(f\"{'Majority Model AUC ROC:':<25} {majority_roc * 100:.1f}%\")\n",
    "print(f\"{'Majority Model Coverage:':<25} {majority_cov * 100:.1f}%\")\n",
    "print(\"++++++++++++++++++++++++\")\n",
    "\n",
    "# add labeling model \n",
    "\n",
    "label_f1 = label_scores.get(\"f1\")\n",
    "label_acc = label_scores.get(\"accuracy\")\n",
    "label_prec = label_scores.get(\"precision\")\n",
    "label_rec = label_scores.get(\"recall\")\n",
    "label_roc = label_scores.get(\"roc_auc\")\n",
    "label_cov = label_scores.get(\"coverage\")\n",
    "print(f\"{'Label Model F1:':<25} {label_f1 * 100:.1f}%\")\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_acc * 100:.1f}%\")\n",
    "print(f\"{'Label Model Precision:':<25} {label_prec * 100:.1f}%\")\n",
    "print(f\"{'Label Model Recall:':<25} {label_rec * 100:.1f}%\")\n",
    "print(f\"{'Label Model AUC ROC:':<25} {label_roc * 100:.1f}%\")\n",
    "print(f\"{'Label Model Coverage:':<25} {label_cov * 100:.1f}%\")\n",
    "\n",
    "# goals\n",
    "# look at acl, nlp conferences, ACL rolling review, umbrella rolling review process for nlp processes\n",
    "# rolling review in June deadline, EM-NLP conference.\n",
    "# Rolling deadline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4c46638-9b8e-4937-8a8e-fd8878554835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy if predicting majority class 0.9149433648550348\n"
     ]
    }
   ],
   "source": [
    "# View \"dummy\" accuracy if predicting majority class every time.\n",
    "print(\"Accuracy if predicting majority class\", \n",
    "      df_dev[\"gold\"].value_counts(normalize = True).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664f846-f93d-4fb7-9133-07349c374308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
