{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cf617-f36a-4e11-99e9-b765ca8aebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# stores all files as series\n",
    "files_as_series = {}\n",
    "# stores first sentence of all files\n",
    "first_sentence_file = {}\n",
    "# stores all POS files as series\n",
    "pos_files_as_series = {}\n",
    "\n",
    "\n",
    "# get index of each token in the .txt file this token is from and returns all indexes as a list.\n",
    "def get_indexes_as_series(tokens):\n",
    "    return tokens.index.tolist()\n",
    "    \n",
    "# returns a list of this value of size n, where each value is the \n",
    "# length of the .txt file this token is from, where n is the length of the input tokens series\n",
    "def get_len_as_series(tokens):\n",
    "    temp = [len(tokens) for i in range(0, len(tokens))]\n",
    "    return temp\n",
    "\n",
    "\n",
    "def file_to_series(file_name):    \n",
    "    # Source: https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/\n",
    "    with open(file_name) as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return pd.Series(lines)\n",
    "\n",
    "# Strip PubMed IDs from file names.\n",
    "strip_pmid = lambda x: x.split(\".\")[0]\n",
    "\n",
    "def iter_token_dir(dir_name, df, label_dict, col_name = \"Token\", ext_name = \".tokens\"):\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            \n",
    "            pos_series_file = file_to_series(directory.decode(\"utf-8\") + file_name.split(\".\")[0] + \".pos\")\n",
    "            \n",
    "            files_as_series[file_name] = series_file\n",
    "            pos_files_as_series[file_name.split(\".\")[0] + \".pos\"] = pos_series_file\n",
    "            \n",
    "            token_index = get_indexes_as_series(series_file)\n",
    "            file_len = get_len_as_series(series_file)\n",
    "                        \n",
    "            PMID = strip_pmid(file_name)\n",
    "            df_file = pd.DataFrame({col_name: series_file,\n",
    "                                    \"File\": [file_name] * len(series_file),\n",
    "                                    \"Gold\": label_dict.get(PMID),\n",
    "                                    \"PMID\": [PMID] * len(series_file),\n",
    "                                    \"token_index\": token_index,\n",
    "                                    \"file_len\": file_len\n",
    "                                   })\n",
    "            df = pd.concat([df, df_file])\n",
    "        else:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "def iter_label_dir(dir_name, ext_name = \".AGGREGATED.ann\"):\n",
    "    label_dict = dict()\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            PMID = strip_pmid(file_name)\n",
    "            label_dict[PMID] = series_file\n",
    "        else:\n",
    "            continue\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "# get sentence index, sentence, and parts of speech of sentence, that token is in\n",
    "def get_sentence_info(token_index, file_name):\n",
    "        \n",
    "    token_series = files_as_series[file_name]\n",
    "    pos_series = pos_files_as_series[file_name.split(\".\")[0] + \".pos\"]\n",
    "    sentence = []\n",
    "    pos_sentence = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    i = token_index\n",
    "    \n",
    "    if token_series[i]=='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "        \n",
    "    while i>=0 and token_series[i]!='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "\n",
    "    # index within sentence\n",
    "    sentence_index = token_index - (i+1)\n",
    "    i = token_index+1 if token_series[token_index]!='.' else token_index\n",
    "\n",
    "    while i<len(token_series) and token_series[i]!='.':\n",
    "        sentence.append(token_series[i])\n",
    "        pos_sentence.append(pos_series[i])\n",
    "        i+=1\n",
    "\n",
    "    if token_index==0:\n",
    "        first_sentence_file[file_name] = [x.lower() for x in sentence]\n",
    "            \n",
    "    return (sentence_index, sentence, pos_sentence)\n",
    "\n",
    "def get_sentence_index(x):\n",
    "    i, s, ps = x\n",
    "    return i\n",
    "def get_sentence(x):\n",
    "    i, s, ps = x\n",
    "    return s\n",
    "def get_pos_sentence(x):\n",
    "    i, s, ps = x\n",
    "    return ps\n",
    "\n",
    "\n",
    "# tokens that are punctuation.\n",
    "def is_punctuation(x):\n",
    "    return False if x.Token.lower() in string.punctuation else True\n",
    "\n",
    "# Iterate through directory to obtain all gold labels, \n",
    "# mapped to their respective file names.\n",
    "label_dict = iter_label_dir(\"annotations/aggregated/starting_spans/interventions/train/\")\n",
    "\n",
    "# Iterate through directory to obtain all tokens,\n",
    "# mapped to their respective file names.\n",
    "# original tokens\n",
    "df_orig = pd.DataFrame()\n",
    "df_orig = iter_token_dir(\"documents/\", df_orig, label_dict)\n",
    "\n",
    "# Remove NA gold labels.\n",
    "df_orig = df_orig.dropna()\n",
    "\n",
    "df_orig = df_orig.reset_index(drop = True)\n",
    "\n",
    "# get sentence related columns for each token\n",
    "df_orig[\"sentence_info\"] = df_orig.apply(lambda x : get_sentence_info(x[\"token_index\"], x[\"File\"]), axis=1)\n",
    "\n",
    "df_orig[\"sentence_index\"] = df_orig[\"sentence_info\"].apply(get_sentence_index)\n",
    "df_orig[\"sentence\"] = df_orig[\"sentence_info\"].apply(get_sentence)\n",
    "df_orig[\"pos_sentence\"] = df_orig[\"sentence_info\"].apply(get_pos_sentence)\n",
    "\n",
    "df_orig = df_orig.drop(\"sentence_info\", 1)\n",
    "\n",
    "\n",
    "\n",
    "def get_hash_sentence(x):\n",
    "    s = ''\n",
    "    for i in x:\n",
    "        s += i\n",
    "    return s\n",
    "\n",
    "df_orig[\"sentence_prep\"] = df_orig[\"sentence\"].apply(get_hash_sentence)\n",
    "\n",
    "# remove punctuation tokens\n",
    "df_orig = df_orig[df_orig.apply(lambda x: is_punctuation(x), axis=1)]\n",
    "\n",
    "df_orig = df_orig.reset_index(drop = True)\n",
    "\n",
    "df_orig = df_orig.sample(n = 10000).reset_index(drop = True)\n",
    "\n",
    "df_orig.to_pickle('df_orig.pickle')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
