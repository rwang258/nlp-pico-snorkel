{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8bdcd3-a972-4d26-a42e-a325fae10db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# stores all files as series\n",
    "files_as_series = {}\n",
    "# stores first sentence of all files\n",
    "first_sentence_file = {}\n",
    "# stores all POS files as series\n",
    "pos_files_as_series = {}\n",
    "\n",
    "\n",
    "# get index of each token in the .txt file this token is from and returns all indexes as a list.\n",
    "def get_indexes_as_series(tokens):\n",
    "    return tokens.index.tolist()\n",
    "    \n",
    "# returns a list of this value of size n, where each value is the \n",
    "# length of the .txt file this token is from, where n is the length of the input tokens series\n",
    "def get_len_as_series(tokens):\n",
    "    temp = [len(tokens) for i in range(0, len(tokens))]\n",
    "    return temp\n",
    "\n",
    "\n",
    "def file_to_series(file_name):    \n",
    "    # Source: https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/\n",
    "    with open(file_name) as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return pd.Series(lines)\n",
    "\n",
    "# Strip PubMed IDs from file names.\n",
    "strip_pmid = lambda x: x.split(\".\")[0]\n",
    "\n",
    "def iter_token_dir(dir_name, df, label_dict, col_name = \"token\", ext_name = \".tokens\"):\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            \n",
    "            pos_series_file = file_to_series(directory.decode(\"utf-8\") + file_name.split(\".\")[0] + \".pos\")\n",
    "            \n",
    "            files_as_series[file_name] = series_file\n",
    "            pos_files_as_series[file_name.split(\".\")[0] + \".pos\"] = pos_series_file\n",
    "            \n",
    "            token_index = get_indexes_as_series(series_file)\n",
    "            file_len = get_len_as_series(series_file)\n",
    "                        \n",
    "            PMID = strip_pmid(file_name)\n",
    "            df_file = pd.DataFrame({col_name: series_file,\n",
    "                                    \"file\": [file_name] * len(series_file),\n",
    "                                    \"gold\": label_dict.get(PMID),\n",
    "                                    \"PMID\": [PMID] * len(series_file),\n",
    "                                    \"token_index\": token_index,\n",
    "                                    \"file_len\": file_len\n",
    "                                   })\n",
    "            df = pd.concat([df, df_file])\n",
    "        else:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "def iter_label_dir(dir_name, ext_name = \".AGGREGATED.ann\"):\n",
    "    label_dict = dict()\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            PMID = strip_pmid(file_name)\n",
    "            label_dict[PMID] = series_file\n",
    "        else:\n",
    "            continue\n",
    "    return label_dict\n",
    "\n",
    "# get abstract, and parts of speech of abstract, for a token\n",
    "def get_abstract_info(token_index, file_name):\n",
    "    \n",
    "    token_series = files_as_series[file_name]\n",
    "    pos_series = pos_files_as_series[file_name.split(\".\")[0] + \".pos\"]\n",
    "    sentence = []\n",
    "    pos_sentence = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    i = token_index\n",
    "    \n",
    "    if token_series[i]=='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "        \n",
    "    while i>=0 and token_series[i]!='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "\n",
    "    # index within sentence\n",
    "    sentence_index = token_index - (i+1)\n",
    "    i = token_index+1 if token_series[token_index]!='.' else token_index\n",
    "\n",
    "    while i<len(token_series) and token_series[i]!='.':\n",
    "        sentence.append(token_series[i])\n",
    "        pos_sentence.append(pos_series[i])\n",
    "        i+=1\n",
    "\n",
    "    if token_index==0:\n",
    "        first_sentence_file[file_name] = [x.lower() for x in sentence]\n",
    "            \n",
    "    return (token_series.tolist(), pos_series.tolist())\n",
    "\n",
    "\n",
    "def get_abstract(x):\n",
    "    s, ps = x\n",
    "    return s\n",
    "def get_pos_abstract(x):\n",
    "    s, ps = x\n",
    "    return ps\n",
    "\n",
    "\n",
    "# tokens that are punctuation.\n",
    "def is_punctuation(x):\n",
    "    return False if x.Token.lower() in string.punctuation else True\n",
    "\n",
    "# Iterate through directory to obtain all gold labels, \n",
    "# mapped to their respective file names.\n",
    "label_dict = iter_label_dir(\"pico_datasets/annotations/aggregated/starting_spans/interventions/train/\")\n",
    "\n",
    "# Iterate through directory to obtain all tokens,\n",
    "# mapped to their respective file names.\n",
    "# original tokens\n",
    "df_orig = pd.DataFrame()\n",
    "df_orig = iter_token_dir(\"pico_datasets/documents/\", df_orig, label_dict)\n",
    "\n",
    "# get abstract related columns for each token\n",
    "df_orig[\"abstract_info\"] = df_orig.apply(lambda x : get_abstract_info(x[\"token_index\"], x[\"file\"]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7e67ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-88642823c79b>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_orig = df_orig.drop(\"abstract_info\", 1)\n"
     ]
    }
   ],
   "source": [
    "df_orig[\"abstract\"] = df_orig[\"abstract_info\"].apply(get_abstract)\n",
    "df_orig[\"pos_abstract\"] = df_orig[\"abstract_info\"].apply(get_pos_abstract)\n",
    "\n",
    "df_orig = df_orig.drop(\"abstract_info\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05345c7d-394d-4597-8ebd-f40fa3504c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0150a5de-8c8d-46a3-acc0-bc0f5d3a08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.head(318712)\n",
    "\n",
    "df_orig.to_pickle('df_orig.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82042a54-d694-4ce3-9c7a-7b0015658224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_sentence_file.pickle', 'wb') as handle:\n",
    "    pickle.dump(first_sentence_file, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1739ed5-dc72-4cdd-af82-a6c077bfacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# read the pickle file\n",
    "with open('df_orig.pickle', 'rb') as handle:\n",
    "    df_orig = pickle.load(handle)\n",
    "    \n",
    "# split into dev set and train_test set\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.06, random_state=42)\n",
    "train_test_ix, dev_ix = next(gs.split(df_orig, groups=df_orig.PMID))\n",
    "\n",
    "df_train_test = df_orig.loc[train_test_ix]\n",
    "df_dev = df_orig.loc[dev_ix]\n",
    "\n",
    "df_train_test = df_train_test.reset_index(drop=True)\n",
    "df_dev = df_dev.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# split train_test set itself\n",
    "gs2 = GroupShuffleSplit(n_splits=2, test_size=.1, random_state=42)\n",
    "train_ix, test_ix = next(gs2.split(df_train_test, groups=df_train_test.PMID))\n",
    "\n",
    "df_train = df_train_test.loc[train_ix]\n",
    "df_test = df_train_test.loc[test_ix]\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_train.to_pickle('df_train.pickle')\n",
    "df_test.to_pickle('df_test.pickle')\n",
    "df_dev.to_pickle('df_dev.pickle')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08e0a0fb-d74c-4f8d-8a37-fec226e1b1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.gold[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7fc69b-e6a1-44c2-a392-6274c80d19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 3000\n",
    "# for name, groups in df_orig.groupby('PMID'):\n",
    "#     if i == 0:\n",
    "#         print(name)\n",
    "#         print(groups)\n",
    "#     i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b19ad84-a368-4ede-a4d9-d7fc3aa36a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig[pd.isnull(df_orig['gold'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b04015bf-a2b7-45c5-a7a3-da7799773a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.iloc[10021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596254f-9b1b-4b00-b2eb-2288fbfdb583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
