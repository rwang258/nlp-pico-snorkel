{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8bdcd3-a972-4d26-a42e-a325fae10db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# stores all files as series\n",
    "files_as_series = {}\n",
    "# stores first sentence of all files\n",
    "first_sentence_file = {}\n",
    "# stores all POS files as series\n",
    "pos_files_as_series = {}\n",
    "\n",
    "\n",
    "# get index of each token in the .txt file this token is from and returns all indexes as a list.\n",
    "def get_indexes_as_series(tokens):\n",
    "    return tokens.index.tolist()\n",
    "    \n",
    "# returns a list of this value of size n, where each value is the \n",
    "# length of the .txt file this token is from, where n is the length of the input tokens series\n",
    "def get_len_as_series(tokens):\n",
    "    temp = [len(tokens) for i in range(0, len(tokens))]\n",
    "    return temp\n",
    "\n",
    "\n",
    "def file_to_series(file_name):    \n",
    "    # Source: https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/\n",
    "    with open(file_name) as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return pd.Series(lines)\n",
    "\n",
    "# Strip PubMed IDs from file names.\n",
    "strip_pmid = lambda x: x.split(\".\")[0]\n",
    "\n",
    "def iter_token_dir(dir_name, df, label_dict, col_name = \"Token\", ext_name = \".tokens\"):\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            \n",
    "            pos_series_file = file_to_series(directory.decode(\"utf-8\") + file_name.split(\".\")[0] + \".pos\")\n",
    "            \n",
    "            files_as_series[file_name] = series_file\n",
    "            pos_files_as_series[file_name.split(\".\")[0] + \".pos\"] = pos_series_file\n",
    "            \n",
    "            token_index = get_indexes_as_series(series_file)\n",
    "            file_len = get_len_as_series(series_file)\n",
    "                        \n",
    "            PMID = strip_pmid(file_name)\n",
    "            df_file = pd.DataFrame({col_name: series_file,\n",
    "                                    \"File\": [file_name] * len(series_file),\n",
    "                                    \"Gold\": label_dict.get(PMID),\n",
    "                                    \"PMID\": [PMID] * len(series_file),\n",
    "                                    \"token_index\": token_index,\n",
    "                                    \"file_len\": file_len\n",
    "                                   })\n",
    "            df = pd.concat([df, df_file])\n",
    "        else:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "def iter_label_dir(dir_name, ext_name = \".AGGREGATED.ann\"):\n",
    "    label_dict = dict()\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            PMID = strip_pmid(file_name)\n",
    "            label_dict[PMID] = series_file\n",
    "        else:\n",
    "            continue\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "# get sentence index, sentence, and parts of speech of sentence, that token is in\n",
    "def get_sentence_info(token_index, file_name):\n",
    "        \n",
    "    token_series = files_as_series[file_name]\n",
    "    pos_series = pos_files_as_series[file_name.split(\".\")[0] + \".pos\"]\n",
    "    sentence = []\n",
    "    pos_sentence = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    i = token_index\n",
    "    \n",
    "    if token_series[i]=='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "        \n",
    "    while i>=0 and token_series[i]!='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "\n",
    "    # index within sentence\n",
    "    sentence_index = token_index - (i+1)\n",
    "    i = token_index+1 if token_series[token_index]!='.' else token_index\n",
    "\n",
    "    while i<len(token_series) and token_series[i]!='.':\n",
    "        sentence.append(token_series[i])\n",
    "        pos_sentence.append(pos_series[i])\n",
    "        i+=1\n",
    "\n",
    "    if token_index==0:\n",
    "        first_sentence_file[file_name] = [x.lower() for x in sentence]\n",
    "            \n",
    "    return (sentence_index, sentence, pos_sentence)\n",
    "\n",
    "def get_sentence_index(x):\n",
    "    i, s, ps = x\n",
    "    return i\n",
    "def get_sentence(x):\n",
    "    i, s, ps = x\n",
    "    return s\n",
    "def get_pos_sentence(x):\n",
    "    i, s, ps = x\n",
    "    return ps\n",
    "\n",
    "\n",
    "# tokens that are punctuation.\n",
    "def is_punctuation(x):\n",
    "    return False if x.Token.lower() in string.punctuation else True\n",
    "\n",
    "# Iterate through directory to obtain all gold labels, \n",
    "# mapped to their respective file names.\n",
    "label_dict = iter_label_dir(\"annotations/aggregated/starting_spans/interventions/train/\")\n",
    "\n",
    "# Iterate through directory to obtain all tokens,\n",
    "# mapped to their respective file names.\n",
    "# original tokens\n",
    "df_orig = pd.DataFrame()\n",
    "df_orig = iter_token_dir(\"documents/\", df_orig, label_dict)\n",
    "\n",
    "# Remove NA gold labels.\n",
    "df_orig = df_orig.dropna()\n",
    "\n",
    "df_orig = df_orig.reset_index(drop = True)\n",
    "\n",
    "# get sentence related columns for each token\n",
    "df_orig[\"sentence_info\"] = df_orig.apply(lambda x : get_sentence_info(x[\"token_index\"], x[\"File\"]), axis=1)\n",
    "\n",
    "df_orig[\"sentence_index\"] = df_orig[\"sentence_info\"].apply(get_sentence_index)\n",
    "df_orig[\"sentence\"] = df_orig[\"sentence_info\"].apply(get_sentence)\n",
    "df_orig[\"pos_sentence\"] = df_orig[\"sentence_info\"].apply(get_pos_sentence)\n",
    "\n",
    "df_orig = df_orig.drop(\"sentence_info\", 1)\n",
    "\n",
    "\n",
    "\n",
    "def get_hash_sentence(x):\n",
    "    s = ''\n",
    "    for i in x:\n",
    "        s += i\n",
    "    return s\n",
    "\n",
    "df_orig[\"sentence_prep\"] = df_orig[\"sentence\"].apply(get_hash_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0150a5de-8c8d-46a3-acc0-bc0f5d3a08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.head(1196210)\n",
    "\n",
    "df_orig.to_pickle('df_orig.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82042a54-d694-4ce3-9c7a-7b0015658224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_sentence_file.pickle', 'wb') as handle:\n",
    "    pickle.dump(first_sentence_file, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7fc69b-e6a1-44c2-a392-6274c80d19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1-compartmentopenlinearPKmodelwaschosentodescribeplasmaconcentrationsofclonidine,andbodyweightandPNAweresignificantcovariatesforapparentclearance(CL/F)asfollows:CL/F(L/h)=15.2×[bodyweight(kg)/70](0.75)×[PNA(day)(0.441)/(4.06(0.441)+PNA(day)(0.441))]\n",
      "                 Token             File Gold      PMID  token_index  file_len  \\\n",
      "1196210              A  20484620.tokens    0  20484620          122       259   \n",
      "1196211  1-compartment  20484620.tokens    0  20484620          123       259   \n",
      "1196212           open  20484620.tokens    0  20484620          124       259   \n",
      "1196213         linear  20484620.tokens    0  20484620          125       259   \n",
      "1196214             PK  20484620.tokens    0  20484620          126       259   \n",
      "...                ...              ...  ...       ...          ...       ...   \n",
      "1196280              (  20484620.tokens    0  20484620          192       259   \n",
      "1196281          0.441  20484620.tokens    0  20484620          193       259   \n",
      "1196282              )  20484620.tokens    0  20484620          194       259   \n",
      "1196283              )  20484620.tokens    0  20484620          195       259   \n",
      "1196284              ]  20484620.tokens    0  20484620          196       259   \n",
      "\n",
      "         sentence_index                                           sentence  \\\n",
      "1196210               0  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196211               1  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196212               2  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196213               3  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196214               4  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "...                 ...                                                ...   \n",
      "1196280              70  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196281              71  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196282              72  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196283              73  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "1196284              74  [A, 1-compartment, open, linear, PK, model, wa...   \n",
      "\n",
      "                                              pos_sentence  \\\n",
      "1196210  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196211  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196212  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196213  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196214  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "...                                                    ...   \n",
      "1196280  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196281  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196282  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196283  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "1196284  [DT, JJ, JJ, NN, NNP, NN, VBD, VBN, TO, VB, JJ...   \n",
      "\n",
      "                                             sentence_prep  \n",
      "1196210  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196211  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196212  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196213  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196214  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "...                                                    ...  \n",
      "1196280  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196281  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196282  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196283  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "1196284  A1-compartmentopenlinearPKmodelwaschosentodesc...  \n",
      "\n",
      "[75 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# i = 1000\n",
    "# for name, groups in df_orig.groupby('sentence_prep'):\n",
    "#     if i == 0:\n",
    "#         print(name)\n",
    "#         print(groups)\n",
    "#     i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b04015bf-a2b7-45c5-a7a3-da7799773a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.iloc[10021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596254f-9b1b-4b00-b2eb-2288fbfdb583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
