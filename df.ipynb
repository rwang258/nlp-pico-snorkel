{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8bdcd3-a972-4d26-a42e-a325fae10db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "\n",
    "# stores all files as series\n",
    "files_as_series = {}\n",
    "# stores first sentence of all files\n",
    "first_sentence_file = {}\n",
    "# stores all POS files as series\n",
    "pos_files_as_series = {}\n",
    "\n",
    "\n",
    "# get index of each token in the .txt file this token is from and returns all indexes as a list.\n",
    "def get_indexes_as_series(tokens):\n",
    "    return tokens.index.tolist()\n",
    "    \n",
    "# returns a list of this value of size n, where each value is the \n",
    "# length of the .txt file this token is from, where n is the length of the input tokens series\n",
    "def get_len_as_series(tokens):\n",
    "    temp = [len(tokens) for i in range(0, len(tokens))]\n",
    "    return temp\n",
    "\n",
    "\n",
    "def file_to_series(file_name):    \n",
    "    # Source: https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/\n",
    "    with open(file_name) as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return pd.Series(lines)\n",
    "\n",
    "# Strip PubMed IDs from file names.\n",
    "strip_pmid = lambda x: x.split(\".\")[0]\n",
    "\n",
    "def iter_token_dir(dir_name, df, label_dict, col_name = \"token\", ext_name = \".tokens\"):\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            \n",
    "            pos_series_file = file_to_series(directory.decode(\"utf-8\") + file_name.split(\".\")[0] + \".pos\")\n",
    "            \n",
    "            files_as_series[file_name] = series_file\n",
    "            pos_files_as_series[file_name.split(\".\")[0] + \".pos\"] = pos_series_file\n",
    "            \n",
    "            token_index = get_indexes_as_series(series_file)\n",
    "            file_len = get_len_as_series(series_file)\n",
    "                        \n",
    "            PMID = strip_pmid(file_name)\n",
    "            df_file = pd.DataFrame({col_name: series_file,\n",
    "                                    \"file\": [file_name] * len(series_file),\n",
    "                                    \"gold\": label_dict.get(PMID),\n",
    "                                    \"PMID\": [PMID] * len(series_file),\n",
    "                                    \"token_index\": token_index,\n",
    "                                    \"file_len\": file_len\n",
    "                                   })\n",
    "            df = pd.concat([df, df_file])\n",
    "        else:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "def iter_label_dir(dir_name, ext_name = \".AGGREGATED.ann\"):\n",
    "    label_dict = dict()\n",
    "    directory = os.fsencode(dir_name)\n",
    "    for file in os.listdir(directory):\n",
    "        file_name = os.fsdecode(file)\n",
    "        if file_name.endswith(ext_name): \n",
    "            series_file = file_to_series(directory.decode(\"utf-8\") + file_name)\n",
    "            PMID = strip_pmid(file_name)\n",
    "            label_dict[PMID] = series_file\n",
    "        else:\n",
    "            continue\n",
    "    return label_dict\n",
    "\n",
    "# get abstract, and parts of speech of abstract, for a token\n",
    "def get_abstract_info(token_index, file_name):\n",
    "    \n",
    "    token_series = files_as_series[file_name]\n",
    "    pos_series = pos_files_as_series[file_name.split(\".\")[0] + \".pos\"]\n",
    "    sentence = []\n",
    "    pos_sentence = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    i = token_index\n",
    "    \n",
    "    if token_series[i]=='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "        \n",
    "    while i>=0 and token_series[i]!='.':\n",
    "        sentence.insert(0, token_series[i])\n",
    "        pos_sentence.insert(0, pos_series[i])\n",
    "        i-=1\n",
    "\n",
    "    # index within sentence\n",
    "    sentence_index = token_index - (i+1)\n",
    "    i = token_index+1 if token_series[token_index]!='.' else token_index\n",
    "\n",
    "    while i<len(token_series) and token_series[i]!='.':\n",
    "        sentence.append(token_series[i])\n",
    "        pos_sentence.append(pos_series[i])\n",
    "        i+=1\n",
    "\n",
    "    if token_index==0:\n",
    "        first_sentence_file[file_name] = [x.lower() for x in sentence]\n",
    "            \n",
    "    return (token_series.tolist(), pos_series.tolist())\n",
    "\n",
    "\n",
    "def get_abstract(x):\n",
    "    s, ps = x\n",
    "    return s\n",
    "def get_pos_abstract(x):\n",
    "    s, ps = x\n",
    "    return ps\n",
    "\n",
    "\n",
    "# tokens that are punctuation.\n",
    "def is_punctuation(x):\n",
    "    return False if x.Token.lower() in string.punctuation else True\n",
    "\n",
    "# Iterate through directory to obtain all gold labels, \n",
    "# mapped to their respective file names.\n",
    "label_dict = iter_label_dir(\"pico_datasets/annotations/aggregated/starting_spans/interventions/train/\")\n",
    "\n",
    "# Iterate through directory to obtain all tokens,\n",
    "# mapped to their respective file names.\n",
    "# original tokens\n",
    "df_orig = pd.DataFrame()\n",
    "df_orig = iter_token_dir(\"pico_datasets/documents/\", df_orig, label_dict)\n",
    "\n",
    "# get abstract related columns for each token\n",
    "df_orig[\"abstract_info\"] = df_orig.apply(lambda x : get_abstract_info(x[\"token_index\"], x[\"file\"]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7e67ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-88642823c79b>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_orig = df_orig.drop(\"abstract_info\", 1)\n"
     ]
    }
   ],
   "source": [
    "df_orig[\"abstract\"] = df_orig[\"abstract_info\"].apply(get_abstract)\n",
    "df_orig[\"pos_abstract\"] = df_orig[\"abstract_info\"].apply(get_pos_abstract)\n",
    "\n",
    "df_orig = df_orig.drop(\"abstract_info\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05345c7d-394d-4597-8ebd-f40fa3504c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0150a5de-8c8d-46a3-acc0-bc0f5d3a08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df_orig.head(318712)\n",
    "\n",
    "df_orig.to_pickle('df_orig.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82042a54-d694-4ce3-9c7a-7b0015658224",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_sentence_file.pickle', 'wb') as handle:\n",
    "    pickle.dump(first_sentence_file, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb7fc69b-e6a1-44c2-a392-6274c80d19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3000\n",
    "for name, groups in df_orig.groupby('PMID'):\n",
    "    if i == 0:\n",
    "        print(name)\n",
    "        print(groups)\n",
    "    i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b19ad84-a368-4ede-a4d9-d7fc3aa36a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>file</th>\n",
       "      <th>gold</th>\n",
       "      <th>PMID</th>\n",
       "      <th>token_index</th>\n",
       "      <th>file_len</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pos_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>Intervention</td>\n",
       "      <td>21669557.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>21669557</td>\n",
       "      <td>0</td>\n",
       "      <td>333</td>\n",
       "      <td>[Intervention, to, lower, household, wood, smo...</td>\n",
       "      <td>[NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>to</td>\n",
       "      <td>21669557.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>21669557</td>\n",
       "      <td>1</td>\n",
       "      <td>333</td>\n",
       "      <td>[Intervention, to, lower, household, wood, smo...</td>\n",
       "      <td>[NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>lower</td>\n",
       "      <td>21669557.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>21669557</td>\n",
       "      <td>2</td>\n",
       "      <td>333</td>\n",
       "      <td>[Intervention, to, lower, household, wood, smo...</td>\n",
       "      <td>[NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>household</td>\n",
       "      <td>21669557.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>21669557</td>\n",
       "      <td>3</td>\n",
       "      <td>333</td>\n",
       "      <td>[Intervention, to, lower, household, wood, smo...</td>\n",
       "      <td>[NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>wood</td>\n",
       "      <td>21669557.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>21669557</td>\n",
       "      <td>4</td>\n",
       "      <td>333</td>\n",
       "      <td>[Intervention, to, lower, household, wood, smo...</td>\n",
       "      <td>[NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317378</th>\n",
       "      <td>relapse</td>\n",
       "      <td>14679127.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>14679127</td>\n",
       "      <td>333</td>\n",
       "      <td>338</td>\n",
       "      <td>[Long-term, survival, in, a, phase, III, ,, ra...</td>\n",
       "      <td>[JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317379</th>\n",
       "      <td>of</td>\n",
       "      <td>14679127.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>14679127</td>\n",
       "      <td>334</td>\n",
       "      <td>338</td>\n",
       "      <td>[Long-term, survival, in, a, phase, III, ,, ra...</td>\n",
       "      <td>[JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317380</th>\n",
       "      <td>ovarian</td>\n",
       "      <td>14679127.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>14679127</td>\n",
       "      <td>335</td>\n",
       "      <td>338</td>\n",
       "      <td>[Long-term, survival, in, a, phase, III, ,, ra...</td>\n",
       "      <td>[JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317381</th>\n",
       "      <td>cancer</td>\n",
       "      <td>14679127.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>14679127</td>\n",
       "      <td>336</td>\n",
       "      <td>338</td>\n",
       "      <td>[Long-term, survival, in, a, phase, III, ,, ra...</td>\n",
       "      <td>[JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317382</th>\n",
       "      <td>.</td>\n",
       "      <td>14679127.tokens</td>\n",
       "      <td>None</td>\n",
       "      <td>14679127</td>\n",
       "      <td>337</td>\n",
       "      <td>338</td>\n",
       "      <td>[Long-term, survival, in, a, phase, III, ,, ra...</td>\n",
       "      <td>[JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13109 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               token             file  gold      PMID  token_index  file_len  \\\n",
       "4971    Intervention  21669557.tokens  None  21669557            0       333   \n",
       "4972              to  21669557.tokens  None  21669557            1       333   \n",
       "4973           lower  21669557.tokens  None  21669557            2       333   \n",
       "4974       household  21669557.tokens  None  21669557            3       333   \n",
       "4975            wood  21669557.tokens  None  21669557            4       333   \n",
       "...              ...              ...   ...       ...          ...       ...   \n",
       "317378       relapse  14679127.tokens  None  14679127          333       338   \n",
       "317379            of  14679127.tokens  None  14679127          334       338   \n",
       "317380       ovarian  14679127.tokens  None  14679127          335       338   \n",
       "317381        cancer  14679127.tokens  None  14679127          336       338   \n",
       "317382             .  14679127.tokens  None  14679127          337       338   \n",
       "\n",
       "                                                 abstract  \\\n",
       "4971    [Intervention, to, lower, household, wood, smo...   \n",
       "4972    [Intervention, to, lower, household, wood, smo...   \n",
       "4973    [Intervention, to, lower, household, wood, smo...   \n",
       "4974    [Intervention, to, lower, household, wood, smo...   \n",
       "4975    [Intervention, to, lower, household, wood, smo...   \n",
       "...                                                   ...   \n",
       "317378  [Long-term, survival, in, a, phase, III, ,, ra...   \n",
       "317379  [Long-term, survival, in, a, phase, III, ,, ra...   \n",
       "317380  [Long-term, survival, in, a, phase, III, ,, ra...   \n",
       "317381  [Long-term, survival, in, a, phase, III, ,, ra...   \n",
       "317382  [Long-term, survival, in, a, phase, III, ,, ra...   \n",
       "\n",
       "                                             pos_abstract  \n",
       "4971    [NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...  \n",
       "4972    [NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...  \n",
       "4973    [NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...  \n",
       "4974    [NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...  \n",
       "4975    [NN, TO, VB, NN, NN, NN, NN, IN, NNP, NNS, NNP...  \n",
       "...                                                   ...  \n",
       "317378  [JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...  \n",
       "317379  [JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...  \n",
       "317380  [JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...  \n",
       "317381  [JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...  \n",
       "317382  [JJ, NN, IN, DT, NN, NNP, ,, VBD, NN, IN, JJ, ...  \n",
       "\n",
       "[13109 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig[pd.isnull(df_orig['gold'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b04015bf-a2b7-45c5-a7a3-da7799773a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.iloc[10021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596254f-9b1b-4b00-b2eb-2288fbfdb583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
