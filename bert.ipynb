{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1276f9e-da35-4d8a-a006-0287e31dc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources\n",
    "# https://stackoverflow.com/questions/11303225/how-to-remove-multiple-indexes-from-a-list-at-the-same-time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4128381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, BertTokenizerFast\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d92051-4f85-41d2-8963-a12eef9127a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, abstracts, labels, tokenizer, max_len):\n",
    "        self.len = len(abstracts)\n",
    "        self.abstracts = abstracts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.abstracts[index]\n",
    "        word_labels = self.labels[index]\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_split_into_words=True,\n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "\n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "\n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                # overwrite label\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4900cb-8b2c-4614-9952-4625d47d3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tokenizer, abstracts_tr, labels_tr, abstracts_test, labels_test, MAX_LEN, tag2idx, bs, random_seed=42):\n",
    "\n",
    "    \n",
    "    training_set = dataset(abstracts_tr, labels_tr, tokenizer, MAX_LEN)\n",
    "    testing_set = dataset(abstracts_test, labels_test, tokenizer, MAX_LEN)\n",
    "    \n",
    "    \n",
    "    train_params = {'batch_size': bs,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "\n",
    "    test_params = {'batch_size': bs,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "    \n",
    "    print(len(tokenizer))\n",
    "    \n",
    "    return training_loader, testing_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea2fff1c-6d83-490b-b32b-5d871e769b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, epochs=5, dropout=0.3):\n",
    "\n",
    "    # initialize model\n",
    "#     model = BertForTokenClassification.from_pretrained(\n",
    "#         \"emilyalsentzer/Bio_ClinicalBERT\",  # bert-base-cased\n",
    "#         num_labels=len(labels_to_ids),\n",
    "#     )\n",
    "    \n",
    "    model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(labels_to_ids))\n",
    "\n",
    "#     model.cuda()\n",
    "    model.to(device)\n",
    "    \n",
    "    print(model.config.vocab_size)\n",
    "\n",
    "\n",
    "    FULL_FINETUNING = True\n",
    "    if FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=3e-5,\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )   \n",
    "\n",
    "\n",
    "    ## Store the average loss after each epoch so we can plot them.\n",
    "    loss_values, validation_loss_values = [], []\n",
    "\n",
    "    accuracy_values, F1_values, precision_values, recall_values = [], [], [], []\n",
    "\n",
    "    done = False\n",
    "    for e in trange(epochs, desc=\"Epoch\"):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        model.train()\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "        # Training loop\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            # Always clear any previously calculated gradients before performing a backward pass.\n",
    "            model.zero_grad()\n",
    "            # forward pass\n",
    "            # This will return the loss (rather than the model output)\n",
    "            # because we have provided the `labels`.\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            # get the loss\n",
    "            loss = outputs[0]\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            total_loss += loss.item()\n",
    "            # Clip the norm of the gradient\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        # Put the model into evaluation mode\n",
    "        model.eval()\n",
    "        # Reset the validation loss for this epoch.\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions , true_labels = [], []\n",
    "        for batch in valid_dataloader:\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            # Telling the model not to compute or store gradients,\n",
    "            # saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # This will return the logits rather than the loss because we have not provided labels.\n",
    "                outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            \n",
    "            eval_loss += outputs[0].mean().item()\n",
    "            \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = outputs[1].view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            curr_predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            true_labels.extend(labels)\n",
    "            predictions.extend(curr_predictions)\n",
    "            \n",
    "            abs_ids = ids.tolist()\n",
    "            # print out an input abstract, abstract's gold labels, and BERT's predictions\n",
    "            if e == 0 and not done and len(abs_ids[0]) > 3 and ('evaluation' in tokenizer.decode(abs_ids[0][1])) and ('of' in tokenizer.decode(abs_ids[0][2])) and ('three' in tokenizer.decode(abs_ids[0][3])):\n",
    "                [print(tokenizer.decode(example), '\\n') for example in ids.tolist()[0]]\n",
    "                [print(flattened_targets.tolist())]\n",
    "                print('********************')\n",
    "                [print(flattened_predictions.tolist())]\n",
    "                done = True\n",
    "        \n",
    "        valid_tags = [ids_to_labels[id.item()] for id in true_labels]\n",
    "        pred_tags = [ids_to_labels[id.item()] for id in predictions]\n",
    "\n",
    "        eval_loss = eval_loss / len(valid_dataloader)\n",
    "        validation_loss_values.append(eval_loss)\n",
    "        print(\"Validation loss: {}\".format(eval_loss))\n",
    "\n",
    "        accuracy = accuracy_score(valid_tags, pred_tags)\n",
    "        print(\"Validation Accuracy: {}\".format(accuracy))\n",
    "        accuracy_values.append(accuracy)\n",
    "\n",
    "        precision = precision_score(valid_tags, pred_tags, labels=['0', '1'], average='macro')\n",
    "        print(\"Validation Precision-Score: {}\".format(precision))\n",
    "        precision_values.append(precision)\n",
    "\n",
    "        recall = recall_score(valid_tags, pred_tags, labels=['0', '1'], average='macro')\n",
    "        print(\"Validation Recall-Score: {}\".format(recall))\n",
    "        recall_values.append(recall)\n",
    "\n",
    "        F1 = f1_score(valid_tags, pred_tags, labels=['0', '1'], average='macro')\n",
    "        print(\"Validation F1-Score: {}\".format(F1))\n",
    "        F1_values.append(F1)\n",
    "        print()\n",
    "\n",
    "\n",
    "    # plotting the metrics\n",
    "\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "    # Plot the learning curve.\n",
    "    plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "    plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
    "    plt.plot(accuracy_values, 'g-o', label=\"accuracy\")\n",
    "    plt.plot(precision_values, 'c-o', label=\"precision\")\n",
    "    plt.plot(recall_values, 'm-o', label=\"recall\")\n",
    "    plt.plot(F1_values, 'y-o', label=\"F1\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return accuracy, precision, recall, F1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e2b20b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-24aebc5b4efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# reading in dataset which contains all training and test tokens, and info for every token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_train.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_test.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m_frombuffer\u001b[0;34m(buf, dtype, shape, order)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_frombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main program\n",
    "\n",
    "# reading in dataset which contains all training and test tokens, and info for every token.\n",
    "df_train = pd.read_pickle('df_train.pickle')\n",
    "df_test = pd.read_pickle('df_test.pickle')\n",
    "\n",
    "\n",
    "# groups tokens into their corresponding abstracts. groups labels into their corresponding abstracts.\n",
    "def abstract_labels(df_orig):\n",
    "    abstracts = []\n",
    "    labels = []\n",
    "    df_abs_groups = df_orig.groupby('PMID')\n",
    "    for name, group in df_abs_groups: # no order\n",
    "        group_labs = group.gold.tolist()\n",
    "        s = group.head(1).abstract.tolist()[0]\n",
    "        \n",
    "        # remove None type values from label and abstracts. Snorkel does the same.\n",
    "        remove_indices = [i for i, v in enumerate(group_labs) if v == None]\n",
    "        group_labs = [i for j, i in enumerate(group_labs) if j not in remove_indices]\n",
    "        s = [i for j, i in enumerate(s) if j not in remove_indices]\n",
    "        \n",
    "        abstracts.append(s)\n",
    "        labels.append(group_labs)\n",
    "    \n",
    "    return abstracts, labels\n",
    "\n",
    "# two corresponding lists of abstracts and labels for each abtract\n",
    "abstracts_tr, labels_tr = abstract_labels(df_train)\n",
    "\n",
    "# two corresponding lists of abstracts and labels for each abtract\n",
    "abstracts_test, labels_test = abstract_labels(df_test)\n",
    "\n",
    "\n",
    "# define parameters\n",
    "MAX_LEN = 512\n",
    "bs = 1\n",
    "\n",
    "# define labels\n",
    "tag_values = ['0', '1']\n",
    "labels_to_ids = {'0': 0, '1': 1}\n",
    "ids_to_labels = {0: '0', 1: '1'}\n",
    "\n",
    "\n",
    "# device set up\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# prepare data\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') # bert-base-un\n",
    "# tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT',\n",
    "#                                           do_lower_case=False)\n",
    "\n",
    "train_dataloader, test_dataloader = process_data(tokenizer, abstracts_tr, labels_tr, \n",
    "                                                  abstracts_test, labels_test, \n",
    "                                                  MAX_LEN, labels_to_ids, bs,\n",
    "                                                  random_seed=42)\n",
    "\n",
    "# train\n",
    "accuracy, precision, recall, F1 = train(train_dataloader, test_dataloader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3022496-4bc1-49f0-ab49-1b1df020ac98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
